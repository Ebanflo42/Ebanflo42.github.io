<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <meta name="keywords" content="Eben,Kadile,Cowley,Mathematics,Math,4th,Dimension,4D,Platonic,Solids,Octaplex,24-Cell,Geometry" />
    <link rel="stylesheet" type="text/css" href="../../style.css" />
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	tex2jax: {inlineMath: [["$", "$"],["\\(","\\)"]]},
	errorSetting: {message: undefined}
});
</script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.6/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <title>
        ML-music-analysis - Eben Kadile
    </title>
</head>

<body>
    <div class="heading">
        <h1>Machine Learning and Music</h1>
        <h2>Training machines to predict and generate music in the simplest possible way.</h2>
        <h3>eben.kadile24@gmail.com</h3>
        <br>
    </div>
    <div align="center" style="width:100%">
        <a class="navItem" style="border-top-left-radius:15px;border-bottom-left-radius:15px;" href="../../../index.html">Home</a>
        <a class="navItem" href="../../../Art.html">Art</a>
        <a class="navItem" href="../../../Software.html">Software</a>
        <a class="navItem" href="../../../Blog.html">Blog</a>
        <a class="navItem" style="border-top-right-radius:15px;border-bottom-right-radius:15px;" href="../../../Research.html">Research</a>
    </div>
    <br>
    <p class="textContent">
        The synthesis and enjoyment of music has been a ubiquitous human activity since the dawn of history. During my time at CATNIP Lab, one of the questions we asked was how machines may be able to gain an understanding of music. Specifically, I trained both linear models and recurrent neural networks using SciKit-Learn and PyTorch to predict the next chord in a piano song, given the last several chords. The results varied from obnoxious to extremely fascinating.
        <br><br>
        There are four standard datasets of piano music which have been used to benchmark recurrent neural networks in the past: JSB_Chorales (JSB standing for Johanne Sebastian Bach), Nottingham, MuseData, and Piano_midi. These datasets were used, for example, in <a href="https://arxiv.org/abs/1206.6392" target="_blank">this</a> Bengio paper. They may also be found in <a href="https://github.com/catniplab/ML-music-analysis" target="_blank">my repository</a>, in the form of matlab files.
        <br><br>
        The first step of training neural networks on this data is converting each song into 88 binary sequences. Each of the 88 sequences will correspond to 1 note, and each entry in the sequence will specify whether the note was on or off during that specific beat. Suppose a song is given by the binary sequences $y_{i,t}$, where $i$ specifies the note and $t$ secifies the time, or beat. The goal of our algorithm, is to take $y_{i,0..t-1}$ as input and then output a sequence $p_{i,t}$, specifying the probability that note $i$ is played at beat $t$. We can say the sequence of probabilities $p_{i,t}$ solves the task if it minimizes the cross entropy loss:
        $$L(p, y)=-\sum_t\sum_i y_{i,t}\log(p_{i,t})$$
        This is a classic loss function in machine learning, and essentially represents how surprised one expects to be about the event $y_{i,t}$ assuming probability is $p_{i,t}$. The basic procedure is to use gradient descent, specifically the Adam optimizer, to train a neural network to output a suitable $p_{i,t}$ given $y_{i,t}$. I used several common methods to ensure that the networks would learn well, including gradient clipping, orthogonal initialization, and <a href="https://ieeexplore.ieee.org/document/9049080" target="_blank">limit cycle initialization</a>.
        <br><br>
        Limit cycle initialization yielded some interesting results for this task, perhaps due to the fact that these neural network's periodic initial behavior gives them an advantage in "understanding" the periodic structure of music. The following "songs" were generated by a Tanh recurrent neural network initialized to a set of orthogonal limit cycles and trained on the JSB_Chorales dataset. The networks were simply given an initial input, and then told to predict the next set of chords based on their last output:
        <br>
    </p>
    <div align="center">
    <audio controls>
        <source src="../../tunes/cyberbach1.wav">
    </audio>
    <audio controls>
        <source src="../../tunes/cyberbach4.wav">
    </audio>
    </div>
    <br>
    <p class="textContent">
        It's really interesting to note that with these two examples the only input that was given was weak Gausian noise and their own output from previous time steps. The quasi-musical structure which they produce is totally due to the synaptic weights learned during training. I'm not actually sure how the network figured out how to make those strange laser-gun sounds from the piano notes.
        <br><br>
        Unfortunately, not every network turned out to be able to produce tunes this interesting. Some of them were more creepy, and bordering on obnoxious. If we perform the same procedure, but for a network trained on the Nottingham dataset, we get something like this:
        <br>
    </p>
    <div align="center">
    <audio controls>
        <source src="../../tunes/cybernottingham.wav">
    </audio>
    </audio>
    </div>
    <p class="textContent">
        This one didn't turn out as well, possibly because the Nottingham dataset is more highly biased towards high notes and repetitive chord progressions. In other words, the first network might have had it easier because it was able to mimic the innate talent of Bach :)
        <br><br>
        In order to use my framework to generate your own weird AI music, clone the repository, install the dependencies specified in the README, and run <code>train_example.py</code> to train a new network, or <code>music_example.py</code> to generate a new piece of music based on a pre-existing network.
    </p>
</body>

</html>