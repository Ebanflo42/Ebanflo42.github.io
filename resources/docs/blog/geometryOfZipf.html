<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta name="keywords" content="Eben,Kadile,Cowley,Graphics,Mathematics,Math,Julia,Zipf,Law,Geometry,Fourier,Laplace,Beltrami,Weyl" />
    <link rel="stylesheet" type="text/css" href="../../style.css" />
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	tex2jax: {inlineMath: [["$", "$"],["\\(","\\)"]]},
	errorSetting: {message: undefined}
});
</script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.6/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <title>
        The Geometry of Zipf's Law - Eben Kadile
    </title>
</head>

<body>
    <div class="heading">
        <h1>The Geometry of Zipf's Law</h1>
        <h2>The precise balance between detail and redundancy.</h2>
        <h3>eben.kadile24@gmail.com</h3>
    </div>
    <br>
    <div align="center" style="width:100%">
        <a class="navItem" style="border-top-left-radius:15px;border-bottom-left-radius:15px;" href="../../../index.html">Home</a>
        <a class="navItem" href="../../../Art.html">Art</a>
        <a class="navItem" href="../../../Software.html">Software</a>
        <a class="navItem" href="../../../Blog.html">Blog</a>
        <a class="navItem" style="border-top-right-radius:15px;border-bottom-right-radius:15px;" href="../../../Research.html">Research</a>
    </div>
    <p class="textContent" id="Introduction">
        <big><u><b>Introduction</b>></u></big>
        <br><br>
        Zipf's law is a seemingly mysterious phenomenon which has its origins in linguistics,
        but which is now known to arise in disparate domains including sociology, geology,
        genomics, physics, and even neuroscience. To state the law as it was originally observed
        for languages, let $f_n$ be the frequency (or total number of occurrences) of the nth most
        common word for some language. Then we have the following approximate equation:
        <br><br>
        $$f_n=\frac{f_1}{n}$$
        <br><br>
        Or, taking the logarithm for a more convenient form:
        <br><br>
        $$\log(f_n)=-\log(n)+\log(f_1)$$
        <br><br>
        For example, the most common word in English is "the," the second most common word is "be,"
        and the third most common word is "to." If this law holds, we would expect "be" to
        occur 1/2 as often as "the," and "to" to occur 1/3 as often as "the."
        <br><br>
        In reality, this is not exactly the case. Zipf's law only becomes accurate
        <i>asymptotically,</i> meaning the equations are only really accurate if $n$ is large.
        To demonstrate this, I performed the analysis on the NIV Bible. I plotted my results below,
        and you can find my code, which includes preprocessing a PDF and cleaning the data,
        <a href="https://github.com/Ebanflo42/zipf-bible" target="_blank">here.</a>
        </p>
        <div align="center">
            <img src="../../images/zipf_curve.png" style="border-radius: 5px; filter: brightness(90%)"/>
        </div>
        <p class="textContent">
            The horizontal axis of this plot corresponds to the logarithm of the nth most common word
            in the Bible, while the vertical axis corresponds to the logarithm of number of occurrences
            of the word. The orange line is the line of best fit for the most common thousand words,
            and the $R^2$ value of $1.00$ signifies that the error of the fit is negligible compared to
            the variance of the data (the statisticians reading this might be skeptical based on the plot,
            but remember that there is a lot more data towards the right).
            <br><br>
            Something which probably stands out at this point is that the many authors of the Bible most
            probably had no idea about exponents, logarithms, lines of best fit, or even equations.
            Indeed, this distribution of words is something that arose completely naturally, and has been
            observed across many different corpuses of text in many different languages. Not only that,
            this distribution has been observed in the magnitude of Earthquakes, the noise spectrum of
            electrical circuits, the sizes of cities, and even the frequency of gene sequences in the Zebra fish.
            <br><br>
            Although the ubiquity of this distribution may be initially surprising, there is a significant
            body of literature across different fields which provides a number of different explanations
            for the phenomenon. However, there is a specific paper from computational neuroscience,
            <a href="https://www.nature.com/articles/s41586-019-1346-5" target="_blank">Stringer and Pachitariu et al.</a>,
            that contains a proof in its supplementary material which I believe gets to the heart of the phenomenon
            in a more elegant and enlightening way than anything else I have read so far.
            <br><br>
            In order to understand the reasoning of Stringer and Pachitariu, we are going to have to review
            some basic statistics before delving into differential geometry, generalizing the Fourier transform,
            and tying it all back to the nature of <i>representation</i> itself, so buckle up and get ready for
            some spicy math.

        </p>
        <p class="textContent" id="PCA">
            <big><u><b>Principal Component Analysis</b></u></big>
            <br><br>
            Suppose we have an experiment which we perform that gives us some vector-valued observations
            $X_1,X_2,...X_N$ in $\mathbb{R}^n$. The most basic statistic we could extract from our data is
            the mean:
            <br><br>
            $$\mu=\frac{1}{N}\sum_{i=1}^N X_i$$
            <br>
            which tells us about the general position of the data. It would be interesting to have an idea
            about how the data is <i>spread out</i> around the mean. If each $X_i$ were a single real number,
            we could compute this using the <i>variance</i>:
            <br><br>
            $$\sigma^2=\frac{1}{N}\sum_{i=1}^N(X_i-\mu)^2$$
            <br>
            However, we need a precise notion of what it means to square a vector. Usually, we would use the <i>inner
            product</i> for this sort of thing. That would give use a real-valued measure of the spread of the data,
            but what we actually want is a measure of the spread in <i>all directions</i>. For this, we use the
            <b>outer product</b>: $\otimes$. $u\otimes v$, for vectors $u$ and $v$, produces a matrix $A$ defined
            by $A_{ij}=u_i v_j$. Then we define the <b>sample covariance matrix</b> as follows:
            <br><br>
            $$\Sigma_S=\frac{1}{N}\sum_{i=1}^N (X_i-\mu)\otimes(X_i-\mu)$$
            <br>
            We should interpret the matrix entry $\Sigma_{Sij}$ as how likely dimension $i$ of the data is likely
            to be spread out when dimension $j$ of the data is spread out.
            <br><br>
            Note that $\Sigma_S$ is symmetric. That's because $\otimes$ always produces symmetric matrices,
            and any sum of symmetric matrices is symmetric. Recall from linear algebra that a symmetric matrix
            may be written in the following form:
            <br><br>
            $$\Sigma_S=U\Lambda U^T$$
            <br>
            Where $\Lambda$ is a non-negative diagonal matrix of eigenvalues and $U$ is a real orthogonal matrix of
            eigenvectors. Now, what we may do at this point is transform our data into a new coordinate system
            $X'_1=U^T X_1,\; X'_2=U^T X_2,...\;X'_N=U^TX_N$. The advantage of this coordinate system is that
            the sample covariance matrix $\Sigma_S'$ of the new data is simply $\Lambda$.
            <br><br>
            What this tells us is that the columns of $U$ can be thought of as an orthogonal basis determining
            the directions in which the data is primarily spread. In practice, a column $U_i$ is called the ith
            <b>principal component</b>, and the eigenvalue $\Lambda_{ii}$ is called the <i>variance along the
            ith principal component</i>.
            <br><br>
            In order to better illustrate what's going on here, lets consider a simple example. Suppose random
            $X_1$ and $X_2$ are sampled from a multivariate Gaussian distribution function:
            $(X_1, X_2)~\mathcal{N}(\mu,\Sigma)$. The probability density function of this distribution is:
            <br><br>
            $$f(\mathbf{x})=\frac{1}{Z}\exp\big[-\frac{1}{2}(\mathbf{x}-\mu)\Sigma^{-1}(\mathbf{x}-\mu)^T\big]$$
            <br>
            Where $Z$ is a normalizing factor which ensures that probability integrated over the entire plane is 1.
            This distribution function is exactly analogous to the 1-dimensional Gaussian distribution, except that
            the variance $\sigma$ is replaced with the covariance matrix $\Sigma$. The inverted covariance $\Sigma^{-1}$
            will scale the input $\mathbf{x}$ by the inverse of its eigenvalues along the axes defined by its
            eigenvectors, meaning larger eigenvalues correspond to larger spread in a sample of the random variable.
            For a Gaussian, the sample covariance $\Sigma_S$ is a good approximation of the true covariance $\Sigma$.
            Let's pick some numbers for our example:
            <br><br>
            $$\Sigma=
            \begin{pmatrix}
            8 & 4 \\
            4 & 5 \\
            \end{pmatrix}$$
            <br>
            There's no special meaning to these numbers, I just fiddled around until I got a nice plot:
        </p>
        <div align="center">
            <img src="../../images/pca_plot.png" style="border-radius: 5px; filter: brightness(90%)"/>
        </div>
        <p class="textContent">
            The blue dots are sampled from a multivariate Gaussian with $\Sigma$ as defined above. The eigenvectors
            of $\Sigma$ are plotted in red and are scaled by their corresponding eigenvalue. As you can see, the
            eigenvectors of $\Sigma$ give us a good idea of the directions in which the data is spread, and the
            eigenvalues give us a good idea of how spread along each direction the data is.
            <br><br>
            In practice, this technique is more commonly used on very high-dimensional data. For every dimension of
            the data, we get a principal component and a corresponding principal variance. To visualize the data,
            we can project it onto the first two or three principal components, and ask how much variance those
            dimensions account for.
            <br><br>
            What Stringer and Pachitariu found is that, if we perform calcium imaging on a mouse's brain while
            showing it videos
        </p>
        <p class="textContent" id="Kernels">
            <big><u><b>Kernels</b></u></big>
            <br><br>
            For the rest of this article, I will invite you to think of functions as vectors. For some readers this
            might be familiar, but for those who are new to the concept remember that for any
            $f,g:\mathbb{R}\rightarrow\mathbb{R}$ we still have that, for all scalars $c\in\mathbb{R}$,
            $x\mapsto f(x)+cg(x)$ is still a function $\mathbb{R}\rightarrow\mathbb{R}$. I will use
            $\mathbb{R}^{\mathbb{N}}$ to denote the vector space of real-valued functions of the natural
            numbers (equivalent to real-valued sequences) and $\mathcal{C}(\mathbb{R}, \mathbb{R})$ to denote
            the vector space of continuous functions from the reals to the reals.
            <br><br>
            Given a real-valued function $f$, we might want to intuitively think of the values $f(x)$ as the
            entries of the vector. We might also want to (very intuitively) think of the integral sign $\int$
            as if it were summation $\sum$. Then suppose we have a function of two variables $K(x,y)$, we could
            think of $K$ as the functional version of a matrix, in which case matrix multiplication of $f$ by $K$
            would be defined as:
            <br><br>
            $$g(x)=\int K(x, y)f(y)dy$$
            <br>
            This is a very dangerous move: it requires that $K(x,\cdot)f(\cdot)$ is an integrable function for all
            $x$, so we can't just throw any old $K$ into this equation. However, if we are careful, the operation
            described above is called an <b>integral operator</b> and $K$ is referred to as its <b>kernel</b>.
            <br><br>
            In a way exactly analogous to how we defined an outer product which takes vectors and produces matrices,
            we can define and outer product which takes functions $f$ and $g$ and produces a kernel $K$:
            <br><br>
            $$K(x,y)=f\otimes g=f(x)g(y)$$
            <br>
            Note that $K(x, y)=K(y,x)$, meaning it is a <b>symmetric</b> kernel.
            <br><br>
            Stringer and Pachitariu's theory rests on the assumption that their calcium images record so many
            neurons (several thousand) in the mouse visual cortex, that the activity space can be thought of as
            infinite-dimensional. That is, we can choose a function space $\mathbb{H}$ for which $f\in\mathbb{H}$
            represents the overall activity of the cortex and the domain of the function indexes the neurons.
            This means that $f(x)$ should be thought of as representing the activity of neuron $x$. Remember
            that their experiments involved showing the mice videos and images, which should be primarily
            responsible for the variance in the activity of the neurons. Suppose we have a finite-dimensional
            space $M$ with points $p\in M$ representing specific stimuli shown to the mouse. Then, assuming the
            brain has some degree of consistency, we autmotically have a <b>representation</b>
            $\Phi:M\rightarrow \mathbb{H}$ which takes stimuli and returns cortex activity.
            <br><br>
            In order to develop a theory of how Zipf's law emerges in the brain, we assume we have a random
            variable $P$ defined on $M$; this denotes the probability distribution over all possible stimuli
            which the mouse could see. I will use $P_1$ and $P_2$ to denote independent samples of $P$.
            Now we can define the <b>covariance kernel</b> on the space of neurons:
            <br><br>
            $$K=\mathbb{E}_{P_1,P_2}[\Phi(P_1)\otimes\Phi(P_2)]$$
            <br>
            Where I used $\mathbb{E}_{P_1,P_2}$ to denote the expectation over two random variables. Intuitively,
            this is nothing more than <i>the infinite-dimensional generalization of the covariance matrix</i>.
            Given two neurons $x$ and $y$, $K(x,y)$ denotes their expected co-activation over all possible different
            stimuli.
        </p>

    </p>
    <div align="center">
        <canvas id="freq2dCanvas" style="height:480px;width:720px"/>
    </div>
    <input type="range" min="-12" max="12" value="2" id="inpxFreq"></input>
    <input type="range" min="-12" max="12" value="-3" id="inpyFreq"></input>
    <br>
    <div align="center">
        <canvas id="freq1dCanvas" style="height:480px;width:720px"/>
    </div>
    <input type="range" min="-12" max="12" value="2" id="inpFreq"></input>
    <br>
    <div align="center">
        <canvas id="highdCanvas" style="height:480px;width:720px"/>
    </div>
    <input type="range" min="0" max="40" value="20" id="expInput"></input>
    <script src="../../scripts/three.min.js"></script>
    <script src="../../scripts/OrbitControls.js"></script>
    <script id="1d_vs" type="glsl">
        void main() {
            gl_Position = projectionMatrix*modelViewMatrix*vec4(position, 1);
        }
    </script>
    <script id="1d_fs" type="glsl">
        void main() { gl_FragColor = vec4(0.2, 0.8, 0.2, 1.0); }
    </script>
    <script id="2d_vs" type="glsl">
        void main() {
          gl_Position = vec4(position, 1.0);
        }
    </script>
    <script id="2d_fs" type="glsl">
        uniform vec2 res;
        uniform vec2 freqs;

        const float pi = 4.0*atan(1.0);

        vec3 colormap(float val, float scale) {
            val /= scale;
            return vec3(max(0.0, val), 0.15*abs(val), -min(0.0, val));
        }

        vec3 render(vec2 coords) {
            vec2 s = sin(2.0*pi*freqs*coords/res);
            vec2 c = cos(2.0*pi*freqs*coords/res);
            return colormap(s.x*s.y, 1.0);
        }

        void main(){
            gl_FragColor = vec4(0, 0, 0, 1);
            for(int i = 0; i < 2; i++){
                for(int j = 0; j < 2; j++){
                    gl_FragColor.rgb += render(gl_FragCoord.xy + vec2(i, j));
                }
            }
            gl_FragColor *= 0.25;
	    }
    </script>
    <script id="highd_vs" type="glsl">
        attribute vec3 vertCol;
        varying vec3 fragCol;
        void main() {
            gl_Position = projectionMatrix*modelViewMatrix*vec4(position, 1);
            fragCol = vertCol;
        }
    </script>
    <script id="highd_fs" type="glsl">
        uniform float p;
        varying vec3 fragCol;
        void main(){
            vec3 col = fragCol/(fragCol.r + fragCol.g + fragCol.b);
            col = pow(col, vec3(-2.0 - p));
            float m = max(col.r, max(col.g, col.b));
            col /= m;
            gl_FragColor = vec4(col, 1);
        }
    </script>
    <script src="../../scripts/displayZipfGeometry.js"></script>
</body>