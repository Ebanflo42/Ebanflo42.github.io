<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta name="keywords" content="Eben,Kadile,Cowley,Graphics,Mathematics,Math,Julia,Zipf,Law,Geometry,Fourier,Laplace,Beltrami,Weyl" />
    <link rel="stylesheet" type="text/css" href="../../style.css" />
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	tex2jax: {inlineMath: [["$", "$"],["\\(","\\)"]]},
	errorSetting: {message: undefined}
});
</script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.6/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <title>
        The Geometry of Zipf's Law - Eben Kadile
    </title>
</head>

<body>
    <div class="heading">
        <h1>The Geometry of Zipf's Law</h1>
        <h2>The precise balance between detail and redundancy.</h2>
        <h3>eben.kadile24@gmail.com</h3>
    </div>
    <br>
    <div align="center" style="width:100%">
        <a class="navItem" style="border-top-left-radius:15px;border-bottom-left-radius:15px;" href="../../../index.html">Home</a>
        <a class="navItem" href="../../../Art.html">Art</a>
        <a class="navItem" href="../../../Software.html">Software</a>
        <a class="navItem" href="../../../Blog.html">Blog</a>
        <a class="navItem" style="border-top-right-radius:15px;border-bottom-right-radius:15px;" href="../../../Research.html">Research</a>
    </div>
    <p class="textContent" id="Introduction">
        <big><u><b>Introduction</b>></u></big>
        <br><br>
        Zipf's law is a seemingly mysterious phenomenon which has its origins in linguistics,
        but which is now known to arise in disparate domains including sociology, geology,
        genomics, physics, and even neuroscience. To state the law as it was originally observed
        for languages, let $f_n$ be the frequency (or total number of occurrences) of the nth most
        common word for some language. Then we have the following approximate equation:
        <br>
        $$f_n=\frac{f_1}{n}$$
        <br><br>
        Or, taking the logarithm for a more convenient form:
        <br>
        $$\log(f_n)=-\log(n)+\log(f_1)$$
        <br><br>
        For example, the most common word in English is "the," the second most common word is "be,"
        and the third most common word is "to." If this law holds, we would expect "be" to
        occur 1/2 as often as "the," and "to" to occur 1/3 as often as "the."
        <br><br>
        In reality, this is not exactly the case. Zipf's law only becomes accurate
        <i>asymptotically,</i> meaning the equations are only really accurate if $n$ is large.
        To demonstrate this, I performed the analysis on the NIV Bible and plotted my results below.
        </p>
        <div align="center">
            <img src="../../images/zipf_curve.png" style="border-radius: 5px; filter: brightness(90%)"/>
        </div>
        <p class="textContent">
            To represent the phenomenon in a very straightforward way, I have plotted on the left a curve proportional to
            $\frac{1}{x}$ as a green dotted line, and shown that it aligns roughly with the frequency of each word with respect
            to its rank. However, this way of plotting a power-law phenonenon is very limited, as it shows us only a few orders
            magnitude of the data. The more common way to plot this is shown on the right, where the logarithm of the number of
            occurrences is plotted with respect to the logarithm of the rank of each word. Transformed to these coordinates,
            the data nearly follows a straight line for 3 orders of magnitude!
            <br><br>
            You can find my code, which includes preprocessing a PDF and cleaning the data,
            <a href="https://github.com/Ebanflo42/zipf-bible" target="_blank">here.</a>
            In my analysis, plural and non-plural words were considered as the same.
            However, different conjugations of the same verb are not identified. I would wager you could get an even better fit
            to a straight line if you considered linguistic artifacts like this.
            <br><br>
            Something which probably stands out at this point is that the many authors of the Bible most
            probably had no idea about exponents, logarithms, lines of best fit, or even equations.
            Indeed, this distribution of words is something that arose completely naturally, and has been
            observed across many different corpuses of text in many different languages. Not only that,
            this distribution has been observed in the
            <a href="https://ieeexplore.ieee.org/abstract/document/4325473" target="_blank">the noise spectrum of
            electrical circuits,</a>
            <a href="https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.013084">
            Earthquake magnitudes, the sizes of cities,</a>
            <a href="https://pubmed.ncbi.nlm.nih.gov/27997544/#&gid=article-figures&pid=fig-4-uid-3">
            and even the frequency of amino acid sequences in the Zebra fish.</a>
            <br><br>
            Although the ubiquity of this distribution may be initially surprising, each paper which I linked
            constains its own explanation for the phenomenon. However, the explanation
            which I want to dive into with this article comes from computational neuroscience, and may even have
            deep consequences for machine learning. The main paper which I'll be referring to is
            <a href="https://www.nature.com/articles/s41586-019-1346-5" target="_blank">Stringer and Pachitariu et al.</a>.
            The main body of this paper shows empirically that the visual cortex of the mouse is exhibiting Zipf's in a
            way which I'll elaborate on in the next section. Contained in the supplement of the paper is a very elegant and
            insightful proof for why we actually <i>expect</i> this distribution to emerge.
            <br><br>
            In order to understand the reasoning of Stringer and Pachitariu, we are going to have to introduce
            some linear algebra before delving into differential geometry, generalizing the Fourier transform,
            and tying it all back to the nature of <i>representation</i> itself, so buckle up and get ready for
            some spicy math!
        </p>
        <p class="textContent" id="Hilbert_Spaces">
            <big><b><u>Hilbert Spaces</u></b></big>
            <br><br>
            For the rest of this article, I will invite you to think of functions as vectors. For some readers this
            might be familiar, but for those who are new to the concept remember that for any
            $f,g:\mathbb{R}\rightarrow\mathbb{R}$ we still have that, for all scalars $c\in\mathbb{R}$,
            $f(x)+cg(x)$ is still a function $\mathbb{R}\rightarrow\mathbb{R}$. We will use the notation
            $\mathcal{C}(S,\mathbb{R})$ to denote the vector space of continuous functions from $S$ to $\mathbb{R}$.
            For example $S$ could be the natural numbers $\mathbb{N}$, an interval $[0,1]$, a square $[0,1]\times [0,1]$,
            or a more complicated space.
            <br><br>
            An extremely important subspace of $\mathcal{C}(\mathbb{R},\mathbb{R})$ is $L^2(\mathbb{R})$.
            This subspace is defined such that for all $f\in L^2(\mathbb{R})$, we have that the quantitity
            <br>
            $$\langle f,f \rangle =\int_{-\infty}^{\infty}|f(x)|^2dx$$
            <br><br>
            is well defined. For general $f,g\in L^2(\mathbb{R})$, we define
            <br>
            $$\langle f,g \rangle =\int_{-\infty}^{\infty}f(x)g(x)dx$$
            <br>
            The operation $\langle\cdot,\cdot\rangle$ is referred to as the inner product on this
            space. A vector space supporting such an operation is referred to as a <b>Hilbert space</b>.
            <br><br>
            To see the intuition for this definition: we might want to think of the value $f(x)$ as the
            entry of the vector at index $x$. We might also want to (very intuitively) think of the integral sign $\int$
            as if it were summation $\sum$. In that case, the operation defined above is precisely the infinite-dimensional
            generalization of the dot product. This is even more clear to see if we think of $L^2(\mathbb{N})$,
            where the equivalent of integration is again summation, so we have that for sequences
            $a,b\in L^2(\mathbb{N})$ the inner product is defined as:
            <br>
            $$\langle a,b \rangle =\sum_{i=0}^{\infty}a_i b_i$$
            which is simply the dot product with infinitely many indices!
        <p class="textContent" id="The_Laplace_Operator">
            <big><b><u>The Laplace Operator</u></b></big>
            <br><br>
            Let's assume from now on that our domain space $S$ is <i>compact</i>, meaning it doesn't go on forever.
            $S$ could be a circle, sphere, line segment, or rectangle, for example. We will also assume that all of our
            functions are <i>zero on the boundary</i>, if a boundary exists. Note that the functions still form a vector
            space with this restriction.
            <br><br>
            One of the most important linear operators which we can define on $L^2(S)$ is the Laplace operator $\Delta$.
            If $S$ is an interval, $[0,a]$, then we have the following definition:
            <br><br>
            $$\Delta=\frac{d}{dx^2}$$
            <br>
            That is, $\Delta$ is simply the second derivative operator. In this case, $\Delta$ measures the <i>concavity</i>,
            of its input function, or how much the function is curving up or down. In the same way we can define an <i>eigenvector
            equation</i> in a finite-dimensional space, we can define an <i>eigenfunction</i> equation for an operator on an
            infinite-dimensional space:
            <br><br>
            $$\Delta v_{\lambda}=\lambda v_{\lambda}$$
            <br>
            In words, this equation is saying that the concavity of $v_{\lambda}(x)$ at $x$ is proportional to $x$  for all $x$. The $sin$
            function is, by construction, the solution to this equation:
            <br><br>
            $$v_{\lambda}(x)=\sin\big(\lambda\sqrt{\frac{\pi}{a}}x\big)$$
            <br>
            Since we are enforcing the condition that $v_{\lambda}$ is $0$ both at $x=0$ and $x=a$, we must have that all eigenvalues are
            of the form $\lambda=n\sqrt{\frac{\pi}{a}}$, where $n$ is an integer.
            <br><br>
            If we turn the interval into a prism, $[0,a]^N$,
            then $\Delta$ is a sum of second derivative operators:
            <br><br>
            $$\Delta=\sum_{k=1}^N \frac{d^2}{dx^2_k}$$
            <br>
        </p>
        </p>
        <p class="textContent" id="Kernels">
            <big><u><b>Kernels</b></u></big>
            <br><br>
            Now that we have defined an inner product for functions, we should think about what it means to have a
            matrix product for functions. Suppose we have a function of two variables $K(x,y)$, we could
            think of $K$ as an infinite-dimensional matrix, with indices $x$ and $y$, in which case matrix
            multiplication of $f$ by $K$ would be defined as:
            <br>
            $$g(x)=\int K(x, y)f(y)dy$$
            <br>
            This is a very dangerous move: it requires that $K(x,\cdot)f(\cdot)$ is an integrable function for all
            $x$, so we can't just throw any old $K$ into this equation. However, if we are careful, the operation
            described above is called an <b>integral operator</b> and $K$ is referred to as its <b>kernel</b>.
            <br><br>
            Stringer and Pachitariu's theory rests on the assumption that their calcium images record so many
            neurons (over ten thousand) in the mouse visual cortex, that the activity space can be thought of as
            infinite-dimensional. That is, we can choose a Hilbert space $\mathbb{H}$ for which $f\in\mathbb{H}$
            represents the overall activity of the cortex and the domain of the function indexes the neurons.
            This means that $f(x)$ should be thought of as representing the activity of neuron $x$. Remember
            that their experiments involved showing the mice videos and images, which should be primarily
            responsible for the variance in the activity of the neurons. Suppose we have a finite-dimensional
            space $S$ with points $s\in S$ representing specific stimuli shown to the mouse. Then, assuming the
            brain has some degree of consistency, we automatically have a <b>representation</b>
            $\Phi:M\rightarrow \mathbb{H}$ which takes stimuli and returns cortex activity.
            <br><br>
            The central object required for developing a theory of how Zipf's law occurs in the brain is a kernel
            $K$ defined on the stimulus space $S$. The definition for $s,s'\in S$ is straightforward:
            <br>
            $$K(s,s')=\langle \Phi(s),\Phi(s') \rangle$$
            <br>
            <br><br>
            To understand this definition, remember that the dot product of two finite-dimensional vectors $\rangle u,v\langle$ can be thought of as a product including
            their mangitudes and their similarity. This still applies in infinite dimensions: $K(s,s')=\langle\Phi(s),\Phi(s')\rangle$
            can be thought of as the similarity of the responses $\Phi(s)$, $\Phi(s')$ of stimuli of $s$ and $s'$.
            <br><br>
            From now on, I will use $K$ to denote specifically this kernel for whatever representation $\Phi$ might be in question.
            In order to understand this definition, remember that our representation $\Phi$ takes a stimulus $s$ to a function describing
            neural activity. We can give a name to that function: $\Phi(s)=\phi(s,c)$ where $\phi(s,c)$ is now the activity of neuron $c$
            given stimulus $s$.
            <br><br>
            In the same way a matrix has an eigenvector equation, a kernel $K$ has an <b>eigenfunction equation</b>:
            <br>
            $$\lambda v(s)=\int_{S}K(s,s')v(s')ds'$$
            <br>
            Where I used the notation $\int_{S}\cdot dS$ to denote integration over $S$.
            <br><br>
            There are two important properties of $K$ to note, which directly follow from the properties of $\langle\cdot ,\cdot\rangle$:
            $\;K(s,s')$ is always non-negative when $s=s'$, and $K(s,s')=K(s',s)$. In finite dimensional spaces, this would allow us to
            say that 1) the eigenvalues are all non-negative and 2) the eigenvectors form an orthonormal basis. It turns out,
            <a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem" target="_blank">Mercer's theorem</a> generalizes this principle
            to infinite dimensions.
            <br><br>
            Specifically, the ith eigenvalue $\lambda_i$ is non-negative and the eigenfunctions $v_i$ and $v_j$ are orthonormal:
            $\int_Sv_i(s)v_j(s)dS$ is 1 if $i=j$ and 0 otherwise. One more thing to note is that Mercer's theorem gives us the
            following representation of the kernel $K$:
            <br>
            $$K(s,s')=\sum_{i=1}^{\infty}\lambda_i v_i(s)v_i(s')$$
            <br>
            This will be an important aspect of our proof later on.
            <br><br>
            Underlining the fact that $S$ is supposed to be a space representing natural images, $\mathbb{H}$ is the space of visual
            cortex activity, and $\Phi$ is the map taking an image and returning brain activity, what Stringer and Pachitariu found
            <b>empirically</b> is that the eigenvalues $\lambda_n$ of the kernel $K$ (which can be computed from sampled data)
            have the following approximate relationship:
            <br>
            $$\lambda_n=\lambda_1 n^{-\alpha}$$
            <br>
            With $\alpha\simeq1$, which is exactly the same relationship appearing in Zipf's law! If you're anything like me, the fact
            that this distribution re-occurs in such an obscure setting is even more confusing than the original phenomenon itself.
            Originally, we were just talking about the frequency of words, but now we're talking about the eigenvalues of some
            infinite-dimensional matrix.
            <br><br>
            However, now that everything is in place, we are about to see why talking about these kernels and their eigenvalues is so
            important!
        </p>
    <div align="center">
        <canvas id="freq2dCanvas" style="height:480px;width:720px"/>
    </div>
    <div align="center">
        <input type="range" min="-12" max="12" value="2" id="inpxFreq"></input>
        <input type="range" min="-12" max="12" value="-3" id="inpyFreq"></input>
    </div>
    <br>
    <div align="center">
        <canvas id="coeff2dCanvas" style="height:360px;width:1440px"/>
    </div>
    <div align="center">
        <input type="range" min="-20" max="20" value="-20" id="inp2dCoeff1"></input>
        <input type="range" min="-20" max="20" value="-20" id="inp2dCoeff2"></input>
        <input type="range" min="-20" max="20" value="20" id="inp2dCoeff3"></input>
    </div>
    <br>
    <div align="center">
        <canvas id="coeff1dCanvas" style="height:360px;width:1440px"/>
    </div>
    <div align="center">
        <input type="range" min="-20" max="20" value="-20" id="inp1dCoeff1"></input>
        <input type="range" min="-20" max="20" value="-20" id="inp1dCoeff2"></input>
        <input type="range" min="-20" max="20" value="20" id="inp1dCoeff3"></input>
    </div>
    <br>
    <div align="center">
        <canvas id="freq1dCanvas" style="height:480px;width:720px"/>
    </div>
    <input type="range" min="-12" max="12" value="2" id="inpFreq"></input>
    <br>
    <div align="center">
        <canvas id="highdCanvas" style="height:480px;width:720px"/>
    </div>
    <input type="range" min="0" max="80" value="20" id="expInput"></input>
    <script src="../../scripts/three.min.js"></script>
    <script src="../../scripts/OrbitControls.js"></script>
    <script id="1d_vs" type="glsl">
        void main() {
            gl_Position = projectionMatrix*modelViewMatrix*vec4(position, 1);
        }
    </script>
    <script id="1d_fs" type="glsl">
        void main() { gl_FragColor = vec4(0.2, 0.8, 0.2, 1.0); }
    </script>
    <script id="2d_vs" type="glsl">
        void main() {
          gl_Position = vec4(position, 1.0);
        }
    </script>
    <script id="2d_fs" type="glsl">
        uniform vec2 res;
        uniform vec2 freqs;

        const float pi = 4.0*atan(1.0);

        vec3 colormap(float val, float scale) {
            return scale*vec3(max(0.0, val), -min(0.0, val), max(0.0, val));
        }

        vec3 render(vec2 coords) {
            vec2 s = sin(pi*freqs*coords/res);
            vec2 c = cos(pi*freqs*coords/res);
            return colormap(s.x*s.y, 0.9);
        }

        void main(){
            gl_FragColor = vec4(0, 0, 0, 1);
            for(int i = 0; i < 2; i++){
                for(int j = 0; j < 2; j++){
                    gl_FragColor.rgb += render(gl_FragCoord.xy + vec2(i, j));
                }
            }
            gl_FragColor *= 0.25;
	    }
    </script>
    <script id="2d_fs2" type="glsl">
        uniform vec2 res;
        uniform vec3 coeffs;

        const float pi = 4.0*atan(1.0);

        const vec2 freqs1 = vec2(1, 2);
        const vec2 freqs2 = vec2(3, 5);
        const vec2 freqs3 = vec2(7, 11);

        vec3 colormap(float val, float scale) {
            return scale*vec3(max(0.0, val), -min(0.0, val), max(0.0, val));
        }

        vec3 render(vec2 coords) {
            if(coords.x < 0.25*res.x) {
                vec2 r = vec2(0.25*res.x, res.y);
                vec2 s = sin(pi*freqs1*coords/r);
                return colormap(s.x*s.y, 0.9);
            }
            else if(coords.x < 0.5*res.x) {
                coords.x -= 0.25*res.x;
                vec2 s = sin(pi*freqs2*coords/res.y);
                return colormap(s.x*s.y, 0.9);
            }
            else if(coords.x < 0.75*res.x) {
                coords.x -= 0.5*res.x;
                vec2 s = sin(pi*freqs3*coords/res.y);
                return colormap(s.x*s.y, 0.9);
            }
            else {
                coords.x -= 0.75*res.x;
                vec2 s1 = sin(pi*freqs1*coords/res.y);
                vec2 s2 = sin(pi*freqs2*coords/res.y);
                vec2 s3 = sin(pi*freqs3*coords/res.y);
                float s = coeffs.x*s1.x*s1.y + coeffs.y*s2.x*s2.y + coeffs.z*s3.x*s3.y;
                return colormap(s, 0.75);
            }
        }

        void main(){
            gl_FragColor = vec4(0, 0, 0, 1);
            for(int i = 0; i < 2; i++){
                for(int j = 0; j < 2; j++){
                    gl_FragColor.rgb += render(gl_FragCoord.xy + vec2(i, j));
                }
            }
            gl_FragColor *= 0.25;
	    }
    </script>
    <script id="highd_vs" type="glsl">
        attribute vec3 vertCol;
        varying vec3 fragCol;
        void main() {
            gl_Position = projectionMatrix*modelViewMatrix*vec4(position, 1);
            fragCol = vertCol;
        }
    </script>
    <script id="highd_fs" type="glsl">
        uniform float p;
        varying vec3 fragCol;
        void main(){
            vec3 col = fragCol/(fragCol.r + fragCol.g + fragCol.b);
            col = pow(col, vec3(-4.0 - p));
            float m = max(col.r, max(col.g, col.b));
            col /= m;
            gl_FragColor = vec4(col, 1);
        }
    </script>
    <script src="../../scripts/displayZipfGeometry.js"></script>
</body>