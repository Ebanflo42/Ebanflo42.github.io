<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta name="keywords" content="Eben,Kadile,Cowley,Graphics,Mathematics,Math,Julia,Zipf,Law,Geometry,Fourier,Laplace,Beltrami,Weyl" />
    <link rel="stylesheet" type="text/css" href="../../style.css" />
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	tex2jax: {inlineMath: [["$", "$"],["\\(","\\)"]]},
	errorSetting: {message: undefined}
});
</script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.6/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <title>
        The Geometry of Zipf's Law - Eben Kadile
    </title>
</head>

<body>
    <div class="heading">
        <h1>The Geometry of Zipf's Law</h1>
        <h2>The precise balance between detail and redundancy.</h2>
        <h3>eben.kadile24@gmail.com</h3>
    </div>
    <br>
    <div align="center" style="width:100%">
        <a class="navItem" style="border-top-left-radius:15px;border-bottom-left-radius:15px;" href="../../../index.html">Home</a>
        <a class="navItem" href="../../../Art.html">Art</a>
        <a class="navItem" href="../../../Software.html">Software</a>
        <a class="navItem" href="../../../Blog.html">Blog</a>
        <a class="navItem" style="border-top-right-radius:15px;border-bottom-right-radius:15px;" href="../../../Research.html">Research</a>
    </div>
    <p class="textContent" id="Introduction">
        <big><u><b>Introduction</b>></u></big>
        <br><br>
        Zipf's law is a seemingly mysterious phenomenon which has its origins in linguistics,
        but which is now known to arise in disparate domains including sociology, geology,
        genomics, physics, and even neuroscience. To state the law as it was originally observed
        for languages, let $f_n$ be the frequency (or total number of occurrences) of the nth most
        common word for some language. Then we have the following approximate equation:
        <br>
        $$f_n=\frac{f_1}{n}$$
        <br><br>
        Or, taking the logarithm for a more convenient form:
        <br>
        $$\log(f_n)=-\log(n)+\log(f_1)$$
        <br><br>
        For example, the most common word in English is "the," the second most common word is "be,"
        and the third most common word is "to." If this law holds, we would expect "be" to
        occur 1/2 as often as "the," and "to" to occur 1/3 as often as "the."
        <br><br>
        In reality, this is not exactly the case. Zipf's law only becomes accurate
        <i>asymptotically,</i> meaning the equations are only really accurate if $n$ is large.
        To demonstrate this, I performed the analysis on the NIV Bible. I plotted my results below,
        and you can find my code, which includes preprocessing a PDF and cleaning the data,
        <a href="https://github.com/Ebanflo42/zipf-bible" target="_blank">here.</a>
        </p>
        <div align="center">
            <img src="../../images/zipf_curve.png" style="border-radius: 5px; filter: brightness(90%)"/>
        </div>
        <p class="textContent">
            The horizontal axis of this plot corresponds to the logarithm of the nth most common word
            in the Bible, while the vertical axis corresponds to the logarithm of number of occurrences
            of the word. The orange line is the line of best fit for the most common thousand words,
            and the $R^2$ value of $1.00$ signifies that the error of the fit is negligible compared to
            the variance of the data (the statisticians reading this might be skeptical based on the plot,
            but remember that there is a lot more data towards the right).
            <br><br>
            Something which probably stands out at this point is that the many authors of the Bible most
            probably had no idea about exponents, logarithms, lines of best fit, or even equations.
            Indeed, this distribution of words is something that arose completely naturally, and has been
            observed across many different corpuses of text in many different languages. Not only that,
            this distribution has been observed in the
            <a href="https://ieeexplore.ieee.org/abstract/document/4325473" target="_blank">the noise spectrum of
            electrical circuits,</a>
            <a href="https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.013084">
            Earthquake magnitudes, the sizes of cities,</a>
            <a href="https://pubmed.ncbi.nlm.nih.gov/27997544/#&gid=article-figures&pid=fig-4-uid-3">
            and even the frequency of amino acid sequences in the Zebra fish.</a>
            <br><br>
            Although the ubiquity of this distribution may be initially surprising, each paper which I linked
            constains its own explanation for the phenomenon. However, the explanation
            which I want to dive into with this article comes from computational neuroscience, and may even have
            deep consequences for machine learning. The main paper which I'll be referring to is
            <a href="https://www.nature.com/articles/s41586-019-1346-5" target="_blank">Stringer and Pachitariu et al.</a>.
            The main body of this paper shows empirically that the visual cortex of the mouse is exhibiting Zipf's in a
            way which I'll elaborate on in the next section. Contained in the supplement of the paper is a very elegant and
            insightful proof for why we actually <i>expect</i> this distribution to emerge.
            <br><br>
            In order to understand the reasoning of Stringer and Pachitariu, we are going to have to review
            some basic statistics before delving into differential geometry, generalizing the Fourier transform,
            and tying it all back to the nature of <i>representation</i> itself, so buckle up and get ready for
            some spicy math!
        </p>
        <!--
        <p class="textContent" id="PCA">
            <big><u><b>Principal Component Analysis</b></u></big>
            <br><br>
            Suppose we have an experiment which we perform that gives us some vector-valued observations
            $X_1,X_2,...X_N$ in $\mathbb{R}^n$. The most basic statistic we could extract from our data is
            the mean:
            <br>
            $$\mu=\frac{1}{N}\sum_{i=1}^N X_i$$
            <br>
            which tells us about the general position of the data. It would be interesting to have an idea
            about how the data is <i>spread out</i> around the mean. If each $X_i$ were a single real number,
            we could compute this using the <i>variance</i>:
            <br>
            $$\sigma^2=\frac{1}{N}\sum_{i=1}^N(X_i-\mu)^2$$
            <br>
            However, we need a precise notion of what it means to square a vector. Usually, we would use the <i>inner
            product</i> for this sort of thing. That would give use a real-valued measure of the spread of the data,
            but what we actually want is a measure of the spread in <i>all directions</i>. For this, we use the
            <b>outer product</b>: $\otimes$. $u\otimes v$, for vectors $u$ and $v$, produces a matrix $A$ defined
            by $A_{ij}=u_i v_j$. Then we define the <b>sample covariance matrix</b> as follows:
            <br>
            $$\Sigma_S=\frac{1}{N}\sum_{i=1}^N (X_i-\mu)\otimes(X_i-\mu)$$
            <br>
            We should interpret the matrix entry $\Sigma_{Sij}$ as how likely dimension $i$ of the data is likely
            to be spread out when dimension $j$ of the data is spread out.
            <br><br>
            Note that $\Sigma_S$ is symmetric. That's because $\otimes$ always produces symmetric matrices,
            and any sum of symmetric matrices is symmetric. Recall from linear algebra that a symmetric matrix
            may be written in the following form:
            <br>
            $$\Sigma_S=U\Lambda U^T$$
            <br>
            Where $\Lambda$ is a non-negative diagonal matrix of eigenvalues and $U$ is a real orthogonal matrix of
            eigenvectors. Now, what we may do at this point is transform our data into a new coordinate system
            $X'_1=U^T X_1,\; X'_2=U^T X_2,...\;X'_N=U^TX_N$. The advantage of this coordinate system is that
            the sample covariance matrix $\Sigma_S'$ of the new data is simply $\Lambda$.
            <br><br>
            What this tells us is that the columns of $U$ can be thought of as an orthogonal basis determining
            the directions in which the data is primarily spread. In practice, a column $U_i$ is called the ith
            <b>principal component</b>, and the eigenvalue $\Lambda_{ii}$ is called the <i>variance along the
            ith principal component</i>.
            <br><br>
            In order to better illustrate what's going on here, lets consider a simple example. Suppose random
            $X_1$ and $X_2$ are sampled from a multivariate Gaussian distribution function:
            $(X_1, X_2)~\mathcal{N}(\mu,\Sigma)$. The probability density function of this distribution is:
            <br>
            $$f(\mathbf{x})=\frac{1}{Z}\exp\big[-\frac{1}{2}(\mathbf{x}-\mu)\Sigma^{-1}(\mathbf{x}-\mu)^T\big]$$
            <br>
            Where $Z$ is a normalizing factor which ensures that probability integrated over the entire plane is 1.
            This distribution function is exactly analogous to the 1-dimensional Gaussian distribution, except that
            the variance $\sigma$ is replaced with the covariance matrix $\Sigma$. The inverted covariance $\Sigma^{-1}$
            will scale the input $\mathbf{x}$ by the inverse of its eigenvalues along the axes defined by its
            eigenvectors, meaning larger eigenvalues correspond to larger spread in a sample of the random variable.
            For a Gaussian, the sample covariance $\Sigma_S$ is a good approximation of the true covariance $\Sigma$.
            Let's pick some numbers for our example:
            <br>
            $$\Sigma=
            \begin{pmatrix}
            8 & 4 \\
            4 & 5 \\
            \end{pmatrix}$$
            <br>
            There's no special meaning to these numbers, I just fiddled around until I got a nice plot:
        </p>
        <div align="center">
            <img src="../../images/pca_plot.png" style="border-radius: 5px; filter: brightness(90%)"/>
        </div>
        <p class="textContent">
            The blue dots are sampled from a multivariate Gaussian with $\Sigma$ as defined above. The eigenvectors
            of $\Sigma$ are plotted in red and are scaled by their corresponding eigenvalue. As you can see, the
            eigenvectors of $\Sigma$ give us a good idea of the directions in which the data is spread, and the
            eigenvalues give us a good idea of how spread along each direction the data is.
            <br><br>
            In practice, this technique is more commonly used on very high-dimensional data. For every dimension of
            the data, we get a principal component and a corresponding principal variance. To visualize the data,
            we can project it onto the first two or three principal components, and ask how much variance those
            dimensions account for.
            <br><br>
            What Stringer and Pachitariu found is that, if they perform calcium imaging on a mouse's brain while
            showing it natural images, and define $\lambda_n$ to be the nth largest eigenvalue of the covariance
            matrix of the mouse's brain activity, then the following approximate relationship emerges empirically:
            <br>
            $$\lambda_n=\frac{\lambda_1}{n}$$
            <br>
            this is exactly the same relationship appearing in Zipf's law!
            <br><br>
            Intuitively, one could think of a "principal component" of a text as
            a single word, since this is a specific variable distinguished from every other word (in the same
            sense that each principal component is orthogonal to every other). Then the nth variance or eigenvalue
            $\lambda_n$ would naturally correspond to the number of occurrences of the word, since that is exactly
            how much the word contributes to the variance of the text.
            <br><br>
            At this point, this most probably seems extremely hand-wavy, and it is. However, if you're willing to
            accept this interpretation for the time being you will later see that there is a very natural reason
            why we would expect this distribution to occur any time there is a representation which must simultaneously
            optimize both its detail and its readability.
            <br><br>
            But before we prove our main theorem, we need a few more definitions.
        </p>
        -->
        <p class="textContent" id="Kernels_and_Hilbert_Spaces">
            <big><u><b>Kernels and Hilbert Spaces</b></u></big>
            <br><br>
            For the rest of this article, I will invite you to think of functions as vectors. For some readers this
            might be familiar, but for those who are new to the concept remember that for any
            $f,g:\mathbb{R}\rightarrow\mathbb{R}$ we still have that, for all scalars $c\in\mathbb{R}$,
            $f(x)+cg(x)$ is still a function $\mathbb{R}\rightarrow\mathbb{R}$. We will use the notation
            $\mathcal{C}(S,\mathbb{R})$ to denote the vector space of continuous functions from $S$ to $\mathbb{R}$.
            For example $S$ could be the natural numbers $\mathbb{N}$, an interval $[0,1]$, a square $[0,1]\times [0,1]$,
            or a more complicated space.
            <br><br>
            An extremely important subspace of $\mathcal{C}(\mathbb{R},\mathbb{R})$ is $L^2(\mathbb{R},\mathbb{R})$.
            This subspace is defined such that for all $f\in L^2(\mathbb{R},\mathbb{R})$, we have that the quantitity
            <br>
            $$\langle f,f \rangle =\int_{-\infty}^{\infty}|f(x)|^2dx$$
            <br><br>
            is well defined. For general $f,g\in L^2(\mathbb{R},\mathbb{R})$, we define
            <br>
            $$\langle f,g \rangle =\int_{-\infty}^{\infty}f(x)g(x)dx$$
            <br>
            The operation $\langle\cdot,\cdot\rangle$ is referred to as the inner product on this
            space. A vector space supporting such an operation is referred to as a <b>Hilbert space</b>.
            <br><br>
            To see the intuition for this definition: we might want to think of the value $f(x)$ as the
            entry of the vector at index $x$. We might also want to (very intuitively) think of the integral sign $\int$
            as if it were summation $\sum$. In that case, the operation defined above is precisely the infinite-dimensional
            generalization of the dot product. This is even more clear to see if we think of $L^2(\mathbb{N}, \mathbb{R})$,
            where the equivalent of integration is again summation, so we have that for sequences
            $a,b\in L^2(\mathbb{N}, \mathbb{R})$ the inner product is defined as:
            <br>
            $$\langle a,b \rangle =\sum_{i=0}^{\infty}a_i b_i$$
            which is simply the dot product with infinitely many indices!
            <br><br>
            Now that we have defined an inner product for functions, we should think about what it means to have a
            matrix product for functions. Suppose we have a function of two variables $K(x,y)$, we could
            think of $K$ as an infinite-dimensional matrix, with indices $x$ and $y$, in which case matrix
            multiplication of $f$ by $K$ would be defined as:
            <br>
            $$g(x)=\int K(x, y)f(y)dy$$
            <br>
            This is a very dangerous move: it requires that $K(x,\cdot)f(\cdot)$ is an integrable function for all
            $x$, so we can't just throw any old $K$ into this equation. However, if we are careful, the operation
            described above is called an <b>integral operator</b> and $K$ is referred to as its <b>kernel</b>.
            <br><br>
            Stringer and Pachitariu's theory rests on the assumption that their calcium images record so many
            neurons (over ten thousand) in the mouse visual cortex, that the activity space can be thought of as
            infinite-dimensional. That is, we can choose a Hilbert space $\mathbb{H}$ for which $f\in\mathbb{H}$
            represents the overall activity of the cortex and the domain of the function indexes the neurons.
            This means that $f(x)$ should be thought of as representing the activity of neuron $x$. Remember
            that their experiments involved showing the mice videos and images, which should be primarily
            responsible for the variance in the activity of the neurons. Suppose we have a finite-dimensional
            space $S$ with points $s\in S$ representing specific stimuli shown to the mouse. Then, assuming the
            brain has some degree of consistency, we automatically have a <b>representation</b>
            $\Phi:M\rightarrow \mathbb{H}$ which takes stimuli and returns cortex activity.
            <br><br>
            The central object required for developing a theory of how Zipf's law occurs in the brain is a kernel
            $K$ defined on the stimulus space $S$. The definition for $s,s'\in S$ is straightforward:
            <br>
            $$K(s,s')=\langle \Phi(s),\Phi(s') \rangle$$
            <br>
            From now on, I will use $K$ to denote specifically this kernel for whatever representation $\Phi$ might be in question.
            In order to understand this definition, remember that our representation $\Phi$ takes a stimulus $s$ to a function describing
            neural activity. We can give a name to that function: $\Phi(s)=\phi(s,c)$ where $\phi(s,c)$ is now the activity of neuron $c$
            given stimulus $s$.
            <br><br>
            In the same way a matrix has an eigenvector equation, a kernel $K$ has an <b>eigenfunction equation</b>:
            <br>
            $$\lambda v(s)=\int_{S}K(s,s')v(s')dS$$
            <br>
            Where I used the notation $\int_{S}\cdot dS$ to denote integration over $S$.
            <br><br>
            There are two important properties of $K$ to note, which directly follow from the properties of $\langle\cdot ,\cdot\rangle$:
            $\;K(s,s')$ is always non-negative when $s=s'$, and $K(s,s')=K(s',s)$. In finite dimensional spaces, this would allow us to
            say that 1) the eigenvalues are all non-negative and 2) the eigenvectors form an orthonormal basis. It turns out,
            <a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem" target="_blank">Mercer's theorem</a> generalizes this principle
            to infinite dimensions.
            <br><br>
            Specifically, the ith eigenvalue $\lambda_i$ is non-negative and the eigenfunctions $v_i$ and $v_j$ are orthonormal:
            $\int_Sv_i(s)v_j(s)dS$ is 1 if $i=j$ and 0 otherwise. One more thing to note is that Mercer's theorem gives us the
            following representation of the kernel $K$:
            <br>
            $$K(s,s')=\sum_{i=1}^{\infty}\lambda_i v_i(s)v_i(s')$$
            <br>
            This will be an important aspect of our proof later on.
            <br><br>
            Underlining the fact that $S$ is supposed to be a space representing natural images, $\mathbb{H}$ is the space of visual
            cortex activity, and $\Phi$ is the map taking an image and returning brain activity, what Stringer and Pachitariu found
            <b>empirically</b> is that the eigenvalues $\lambda_n$ of the kernel $K$ (which can be computed from sampled data)
            have the following approximate relationship:
            <br>
            $$\lambda_n=\lambda_1 n^{-\alpha}$$
            <br>
            With $\alpha\simeq1$, which is exactly the same relationship appearing in Zipf's law!
            <br><br>
            Most of our definitions and observations are now in place, but before proving our main result we need to review a few more theorems.
            <br><br>
            Some short notes for advanced readers: a Hilbert space is actually any inner product space <i>such that the metric
            induced by the inner product is complete.</i> Stringer and Pachitariu specifically refer to <i>separable</i> Hilbert spaces.
            Also, if you are comfortable with the definitions, it is an important exercise to prove that $L^2$ is indeed a
            Hilbert space. Also, its important to be certain that the application of Mercer's theorem was valid.
        </p>
    <div align="center">
        <canvas id="freq2dCanvas" style="height:480px;width:720px"/>
    </div>
    <div align="center">
        <input type="range" min="-12" max="12" value="2" id="inpxFreq"></input>
        <input type="range" min="-12" max="12" value="-3" id="inpyFreq"></input>
    </div>
    <br>
    <div align="center">
        <canvas id="coeff2dCanvas" style="height:360px;width:1440px"/>
    </div>
    <div align="center">
        <input type="range" min="-20" max="20" value="-20" id="inp2dCoeff1"></input>
        <input type="range" min="-20" max="20" value="-20" id="inp2dCoeff2"></input>
        <input type="range" min="-20" max="20" value="20" id="inp2dCoeff3"></input>
    </div>
    <br>
    <div align="center">
        <canvas id="coeff1dCanvas" style="height:360px;width:1440px"/>
    </div>
    <div align="center">
        <input type="range" min="-20" max="20" value="-20" id="inp1dCoeff1"></input>
        <input type="range" min="-20" max="20" value="-20" id="inp1dCoeff2"></input>
        <input type="range" min="-20" max="20" value="20" id="inp1dCoeff3"></input>
    </div>
    <br>
    <div align="center">
        <canvas id="freq1dCanvas" style="height:480px;width:720px"/>
    </div>
    <input type="range" min="-12" max="12" value="2" id="inpFreq"></input>
    <br>
    <div align="center">
        <canvas id="highdCanvas" style="height:480px;width:720px"/>
    </div>
    <input type="range" min="0" max="80" value="20" id="expInput"></input>
    <script src="../../scripts/three.min.js"></script>
    <script src="../../scripts/OrbitControls.js"></script>
    <script id="1d_vs" type="glsl">
        void main() {
            gl_Position = projectionMatrix*modelViewMatrix*vec4(position, 1);
        }
    </script>
    <script id="1d_fs" type="glsl">
        void main() { gl_FragColor = vec4(0.2, 0.8, 0.2, 1.0); }
    </script>
    <script id="2d_vs" type="glsl">
        void main() {
          gl_Position = vec4(position, 1.0);
        }
    </script>
    <script id="2d_fs" type="glsl">
        uniform vec2 res;
        uniform vec2 freqs;

        const float pi = 4.0*atan(1.0);

        vec3 colormap(float val, float scale) {
            return scale*vec3(max(0.0, val), -min(0.0, val), max(0.0, val));
        }

        vec3 render(vec2 coords) {
            vec2 s = sin(pi*freqs*coords/res);
            vec2 c = cos(pi*freqs*coords/res);
            return colormap(s.x*s.y, 0.9);
        }

        void main(){
            gl_FragColor = vec4(0, 0, 0, 1);
            for(int i = 0; i < 2; i++){
                for(int j = 0; j < 2; j++){
                    gl_FragColor.rgb += render(gl_FragCoord.xy + vec2(i, j));
                }
            }
            gl_FragColor *= 0.25;
	    }
    </script>
    <script id="2d_fs2" type="glsl">
        uniform vec2 res;
        uniform vec3 coeffs;

        const float pi = 4.0*atan(1.0);

        const vec2 freqs1 = vec2(1, 2);
        const vec2 freqs2 = vec2(3, 5);
        const vec2 freqs3 = vec2(7, 11);

        vec3 colormap(float val, float scale) {
            return scale*vec3(max(0.0, val), -min(0.0, val), max(0.0, val));
        }

        vec3 render(vec2 coords) {
            if(coords.x < 0.25*res.x) {
                vec2 r = vec2(0.25*res.x, res.y);
                vec2 s = sin(pi*freqs1*coords/r);
                return colormap(s.x*s.y, 0.9);
            }
            else if(coords.x < 0.5*res.x) {
                coords.x -= 0.25*res.x;
                vec2 s = sin(pi*freqs2*coords/res.y);
                return colormap(s.x*s.y, 0.9);
            }
            else if(coords.x < 0.75*res.x) {
                coords.x -= 0.5*res.x;
                vec2 s = sin(pi*freqs3*coords/res.y);
                return colormap(s.x*s.y, 0.9);
            }
            else {
                coords.x -= 0.75*res.x;
                vec2 s1 = sin(pi*freqs1*coords/res.y);
                vec2 s2 = sin(pi*freqs2*coords/res.y);
                vec2 s3 = sin(pi*freqs3*coords/res.y);
                float s = coeffs.x*s1.x*s1.y + coeffs.y*s2.x*s2.y + coeffs.z*s3.x*s3.y;
                return colormap(s, 0.75);
            }
        }

        void main(){
            gl_FragColor = vec4(0, 0, 0, 1);
            for(int i = 0; i < 2; i++){
                for(int j = 0; j < 2; j++){
                    gl_FragColor.rgb += render(gl_FragCoord.xy + vec2(i, j));
                }
            }
            gl_FragColor *= 0.25;
	    }
    </script>
    <script id="highd_vs" type="glsl">
        attribute vec3 vertCol;
        varying vec3 fragCol;
        void main() {
            gl_Position = projectionMatrix*modelViewMatrix*vec4(position, 1);
            fragCol = vertCol;
        }
    </script>
    <script id="highd_fs" type="glsl">
        uniform float p;
        varying vec3 fragCol;
        void main(){
            vec3 col = fragCol/(fragCol.r + fragCol.g + fragCol.b);
            col = pow(col, vec3(-4.0 - p));
            float m = max(col.r, max(col.g, col.b));
            col /= m;
            gl_FragColor = vec4(col, 1);
        }
    </script>
    <script src="../../scripts/displayZipfGeometry.js"></script>
</body>